{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'sms+spam+collection/SMSSpamCollection'\n",
    "\n",
    "df = pd.read_csv(file_path, sep='\\t', header=None, names=['label', 'message'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label      0\n",
       "message    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5572"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing spam label('spam') to 1 and non-spam label('ham') to 0\n",
    "df['label'] = df['label'].replace({'spam': 1, 'ham': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  go until jurong point, crazy.. available only ...\n",
       "1      0                      ok lar... joking wif u oni...\n",
       "2      1  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3      0  u dun say so early hor... u c already then say...\n",
       "4      0  nah i don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# converting messages to lowercases since for instance 'N' is computationally != 'n'\n",
    "df['message'] = df['message'].apply(lambda a: a.lower())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       go until jurong point, crazy.. available only ...\n",
       "1                           ok lar... joking wif u oni...\n",
       "2       free entry in 2 a wkly comp to win fa cup fina...\n",
       "3       u dun say so early hor... u c already then say...\n",
       "4       nah i don't think he goes to usf, he lives aro...\n",
       "                              ...                        \n",
       "5567    this is the 2nd time we have tried 2 contact u...\n",
       "5568                 will ü b going to esplanade fr home?\n",
       "5569    pity, * was in mood for that. so...any other s...\n",
       "5570    the guy did some bitching but i acted like i'd...\n",
       "5571                           rofl. its true to its name\n",
       "Name: message, Length: 5572, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing url's from messages\n",
    "df['message'] = df['message'].str.replace(r'http\\S+', '', regex=True)\n",
    "\n",
    "df['message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenization**\n",
    "* splitting each message to individual words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\ATTAH KUMAH\n",
      "[nltk_data]     MENSAH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\ATTAH KUMAH\n",
      "[nltk_data]     MENSAH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_message(messages):\n",
    "    \"\"\"\n",
    "    tokenize each message to split words so they stand on their own\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "\n",
    "    tokenized_words = word_tokenize(messages)\n",
    "    tokens.extend(tokenized_words)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "df['tokenized'] = df['message'].apply(tokenize_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, until, jurong, point, ,, crazy, .., avail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, so, early, hor, ..., u, c, alrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, i, do, n't, think, he, goes, to, usf, ,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>freemsg hey there darling it's been 3 week's n...</td>\n",
       "      <td>[freemsg, hey, there, darling, it, 's, been, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>as per your request 'melle melle (oru minnamin...</td>\n",
       "      <td>[as, per, your, request, 'melle, melle, (, oru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>winner!! as a valued network customer you have...</td>\n",
       "      <td>[winner, !, !, as, a, valued, network, custome...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>had your mobile 11 months or more? u r entitle...</td>\n",
       "      <td>[had, your, mobile, 11, months, or, more, ?, u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message  \\\n",
       "0      0  go until jurong point, crazy.. available only ...   \n",
       "1      0                      ok lar... joking wif u oni...   \n",
       "2      1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3      0  u dun say so early hor... u c already then say...   \n",
       "4      0  nah i don't think he goes to usf, he lives aro...   \n",
       "5      1  freemsg hey there darling it's been 3 week's n...   \n",
       "6      0  even my brother is not like to speak with me. ...   \n",
       "7      0  as per your request 'melle melle (oru minnamin...   \n",
       "8      1  winner!! as a valued network customer you have...   \n",
       "9      1  had your mobile 11 months or more? u r entitle...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [go, until, jurong, point, ,, crazy, .., avail...  \n",
       "1           [ok, lar, ..., joking, wif, u, oni, ...]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [u, dun, say, so, early, hor, ..., u, c, alrea...  \n",
       "4  [nah, i, do, n't, think, he, goes, to, usf, ,,...  \n",
       "5  [freemsg, hey, there, darling, it, 's, been, 3...  \n",
       "6  [even, my, brother, is, not, like, to, speak, ...  \n",
       "7  [as, per, your, request, 'melle, melle, (, oru...  \n",
       "8  [winner, !, !, as, a, valued, network, custome...  \n",
       "9  [had, your, mobile, 11, months, or, more, ?, u...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized data to saved-data/tokenized_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Saving the tokenized dataframe\n",
    "\n",
    "file_path = 'saved-data/tokenized_data.csv'\n",
    "\n",
    "df.to_csv(file_path, index=False)\n",
    "print(f'Saved tokenized data to {file_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing common `stopwords` such as \"the,\" \"is,\" \"and,\" etc., which do not carry significant meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenize_message):\n",
    "    \"\"\"\n",
    "    removes words that do not have meanings from the tokenized messages\n",
    "    \"\"\"\n",
    "    stop_word = set(stopwords.words('english'))\n",
    "    filtered_words = [word for word in tokenize_message if word not in stop_word]\n",
    "\n",
    "    return filtered_words\n",
    "\n",
    "df[\"tokenized\"] = df[\"tokenized\"].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, ,, crazy, .., available, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joking, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, early, hor, ..., u, c, already, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goes, usf, ,, lives, around,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>freemsg hey there darling it's been 3 week's n...</td>\n",
       "      <td>[freemsg, hey, darling, 's, 3, week, 's, word,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, brother, like, speak, ., treat, like, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>as per your request 'melle melle (oru minnamin...</td>\n",
       "      <td>[per, request, 'melle, melle, (, oru, minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>winner!! as a valued network customer you have...</td>\n",
       "      <td>[winner, !, !, valued, network, customer, sele...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>had your mobile 11 months or more? u r entitle...</td>\n",
       "      <td>[mobile, 11, months, ?, u, r, entitled, update...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message  \\\n",
       "0      0  go until jurong point, crazy.. available only ...   \n",
       "1      0                      ok lar... joking wif u oni...   \n",
       "2      1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3      0  u dun say so early hor... u c already then say...   \n",
       "4      0  nah i don't think he goes to usf, he lives aro...   \n",
       "5      1  freemsg hey there darling it's been 3 week's n...   \n",
       "6      0  even my brother is not like to speak with me. ...   \n",
       "7      0  as per your request 'melle melle (oru minnamin...   \n",
       "8      1  winner!! as a valued network customer you have...   \n",
       "9      1  had your mobile 11 months or more? u r entitle...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [go, jurong, point, ,, crazy, .., available, b...  \n",
       "1           [ok, lar, ..., joking, wif, u, oni, ...]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3  [u, dun, say, early, hor, ..., u, c, already, ...  \n",
       "4  [nah, n't, think, goes, usf, ,, lives, around,...  \n",
       "5  [freemsg, hey, darling, 's, 3, week, 's, word,...  \n",
       "6  [even, brother, like, speak, ., treat, like, a...  \n",
       "7  [per, request, 'melle, melle, (, oru, minnamin...  \n",
       "8  [winner, !, !, valued, network, customer, sele...  \n",
       "9  [mobile, 11, months, ?, u, r, entitled, update...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming or Lemmatization**\n",
    "* Reduce words to their base or root form or to their dictionary form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_words(stopwords_tokenize_message):\n",
    "    \"\"\"\n",
    "    breaking words to a base or it's dictionary form\n",
    "    \"\"\"\n",
    "    stemmed_token = [stemmer.stem(word) for word in stopwords_tokenize_message]\n",
    "    return stemmed_token\n",
    "\n",
    "\n",
    "df['tokenized'] = df['tokenized'].apply(stemming_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>[go, jurong, point, ,, crazi, .., avail, bugi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>[ok, lar, ..., joke, wif, u, oni, ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>[u, dun, say, earli, hor, ..., u, c, alreadi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, n't, think, goe, usf, ,, live, around, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>freemsg hey there darling it's been 3 week's n...</td>\n",
       "      <td>[freemsg, hey, darl, 's, 3, week, 's, word, ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>even my brother is not like to speak with me. ...</td>\n",
       "      <td>[even, brother, like, speak, ., treat, like, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>as per your request 'melle melle (oru minnamin...</td>\n",
       "      <td>[per, request, 'mell, mell, (, oru, minnaminun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>winner!! as a valued network customer you have...</td>\n",
       "      <td>[winner, !, !, valu, network, custom, select, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>had your mobile 11 months or more? u r entitle...</td>\n",
       "      <td>[mobil, 11, month, ?, u, r, entitl, updat, lat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>i'm gonna be home soon and i don't want to tal...</td>\n",
       "      <td>['m, gon, na, home, soon, n't, want, talk, stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>six chances to win cash! from 100 to 20,000 po...</td>\n",
       "      <td>[six, chanc, win, cash, !, 100, 20,000, pound,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>urgent! you have won a 1 week free membership ...</td>\n",
       "      <td>[urgent, !, 1, week, free, membership, £100,00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>i've been searching for the right words to tha...</td>\n",
       "      <td>['ve, search, right, word, thank, breather, .,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>i have a date on sunday with will!!</td>\n",
       "      <td>[date, sunday, !, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>xxxmobilemovieclub: to use your credit, click ...</td>\n",
       "      <td>[xxxmobilemovieclub, :, use, credit, ,, click,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>oh k...i'm watching here:)</td>\n",
       "      <td>[oh, k, ..., 'm, watch, :, )]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>eh u remember how 2 spell his name... yes i di...</td>\n",
       "      <td>[eh, u, rememb, 2, spell, name, ..., ye, ., v,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>fine if thats the way u feel. thats the way ...</td>\n",
       "      <td>[fine, that, way, u, feel, ., that, way, got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>england v macedonia - dont miss the goals/team...</td>\n",
       "      <td>[england, v, macedonia, -, dont, miss, goals/t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label                                            message  \\\n",
       "0       0  go until jurong point, crazy.. available only ...   \n",
       "1       0                      ok lar... joking wif u oni...   \n",
       "2       1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3       0  u dun say so early hor... u c already then say...   \n",
       "4       0  nah i don't think he goes to usf, he lives aro...   \n",
       "5       1  freemsg hey there darling it's been 3 week's n...   \n",
       "6       0  even my brother is not like to speak with me. ...   \n",
       "7       0  as per your request 'melle melle (oru minnamin...   \n",
       "8       1  winner!! as a valued network customer you have...   \n",
       "9       1  had your mobile 11 months or more? u r entitle...   \n",
       "10      0  i'm gonna be home soon and i don't want to tal...   \n",
       "11      1  six chances to win cash! from 100 to 20,000 po...   \n",
       "12      1  urgent! you have won a 1 week free membership ...   \n",
       "13      0  i've been searching for the right words to tha...   \n",
       "14      0                i have a date on sunday with will!!   \n",
       "15      1  xxxmobilemovieclub: to use your credit, click ...   \n",
       "16      0                         oh k...i'm watching here:)   \n",
       "17      0  eh u remember how 2 spell his name... yes i di...   \n",
       "18      0  fine if thats the way u feel. thats the way ...   \n",
       "19      1  england v macedonia - dont miss the goals/team...   \n",
       "\n",
       "                                            tokenized  \n",
       "0   [go, jurong, point, ,, crazi, .., avail, bugi,...  \n",
       "1              [ok, lar, ..., joke, wif, u, oni, ...]  \n",
       "2   [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "3   [u, dun, say, earli, hor, ..., u, c, alreadi, ...  \n",
       "4   [nah, n't, think, goe, usf, ,, live, around, t...  \n",
       "5   [freemsg, hey, darl, 's, 3, week, 's, word, ba...  \n",
       "6   [even, brother, like, speak, ., treat, like, a...  \n",
       "7   [per, request, 'mell, mell, (, oru, minnaminun...  \n",
       "8   [winner, !, !, valu, network, custom, select, ...  \n",
       "9   [mobil, 11, month, ?, u, r, entitl, updat, lat...  \n",
       "10  ['m, gon, na, home, soon, n't, want, talk, stu...  \n",
       "11  [six, chanc, win, cash, !, 100, 20,000, pound,...  \n",
       "12  [urgent, !, 1, week, free, membership, £100,00...  \n",
       "13  ['ve, search, right, word, thank, breather, .,...  \n",
       "14                               [date, sunday, !, !]  \n",
       "15  [xxxmobilemovieclub, :, use, credit, ,, click,...  \n",
       "16                      [oh, k, ..., 'm, watch, :, )]  \n",
       "17  [eh, u, rememb, 2, spell, name, ..., ye, ., v,...  \n",
       "18  [fine, that, way, u, feel, ., that, way, got...  \n",
       "19  [england, v, macedonia, -, dont, miss, goals/t...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/6.8 MB 975.2 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.2/6.8 MB 1.3 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.2/6.8 MB 1.3 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.3/6.8 MB 1.5 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.4/6.8 MB 1.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.5/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.6/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.7/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.8/6.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.9/6.8 MB 1.7 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.0/6.8 MB 1.6 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.0/6.8 MB 1.5 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.1/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.2/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 1.3/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.4/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.4/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.5/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 1.5/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.5/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.6/6.8 MB 1.2 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 1.8/6.8 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.9/6.8 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.9/6.8 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.9/6.8 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 2.0/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 2.1/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 2.1/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------ --------------------------- 2.2/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.2/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.2/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.3/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.3/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.3/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.3/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.3/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.5/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.6/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.6/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.7/6.8 MB 1.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.7/6.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.8/6.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 2.8/6.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 2.9/6.8 MB 1.0 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.1/6.8 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.1/6.8 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 3.2/6.8 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.2/6.8 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.3/6.8 MB 1.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 3.3/6.8 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 3.4/6.8 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 3.6/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 3.7/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 3.8/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.9/6.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 4.0/6.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.1/6.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.1/6.8 MB 1.2 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.1/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.2/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 4.2/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.3/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.3/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.3/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 4.4/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 4.4/6.8 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 4.5/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.6/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.6/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.7/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 4.7/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.7/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.8/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 4.9/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.0/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 5.1/6.8 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 5.2/6.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.3/6.8 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 5.4/6.8 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.4/6.8 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.5/6.8 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 5.5/6.8 MB 1.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.6/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.6/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.7/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.7/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.8/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.9/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.9/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.0/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.0/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 6.1/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.1/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.1/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 6.2/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.4/6.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.5/6.8 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.5/6.8 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.7/6.8 MB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.8/6.8 MB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspellchecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell checking and correction\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "# Initialize spell checker\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Function to correct spelling of a single message\n",
    "def correct_spelling(message):\n",
    "    return [spell.correction(word) if word in spell else word for word in message]\n",
    "\n",
    "# Apply the correction function to all messages in the DataFrame in batches\n",
    "batch_size = 1000\n",
    "num_batches = (len(df) + batch_size - 1) // batch_size\n",
    "\n",
    "for i in range(num_batches):\n",
    "    start_idx = i * batch_size\n",
    "    end_idx = min((i + 1) * batch_size, len(df))\n",
    "    df.loc[start_idx:end_idx, \"tokenized\"] = df.loc[start_idx:end_idx, \"tokenized\"].apply(correct_spelling)\n",
    "\n",
    "# Join the corrected tokens back into a single string\n",
    "df[\"tokenized\"] = df[\"tokenized\"].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go jurong point , crazi .. avail bugi n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar ... joke wif u oni ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say earli hor ... u c alreadi say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah n't think goe usf , live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>freemsg hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey darl 's 3 week 's word back ! 'd l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>even my brother is not like to speak with me. ...</td>\n",
       "      <td>even brother like speak . treat like aid patent .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>as per your request 'melle melle (oru minnamin...</td>\n",
       "      <td>per request 'mell mell ( oru minnaminungint nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>winner!! as a valued network customer you have...</td>\n",
       "      <td>winner ! ! valu network custom select receivea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>had your mobile 11 months or more? u r entitle...</td>\n",
       "      <td>mobil 11 month ? u r entitl updat latest colou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message  \\\n",
       "0      0  go until jurong point, crazy.. available only ...   \n",
       "1      0                      ok lar... joking wif u oni...   \n",
       "2      1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3      0  u dun say so early hor... u c already then say...   \n",
       "4      0  nah i don't think he goes to usf, he lives aro...   \n",
       "5      1  freemsg hey there darling it's been 3 week's n...   \n",
       "6      0  even my brother is not like to speak with me. ...   \n",
       "7      0  as per your request 'melle melle (oru minnamin...   \n",
       "8      1  winner!! as a valued network customer you have...   \n",
       "9      1  had your mobile 11 months or more? u r entitle...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  go jurong point , crazi .. avail bugi n great ...  \n",
       "1                      ok lar ... joke wif u oni ...  \n",
       "2  free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "3        u dun say earli hor ... u c alreadi say ...  \n",
       "4         nah n't think goe usf , live around though  \n",
       "5  freemsg hey darl 's 3 week 's word back ! 'd l...  \n",
       "6  even brother like speak . treat like aid patent .  \n",
       "7  per request 'mell mell ( oru minnaminungint nu...  \n",
       "8  winner ! ! valu network custom select receivea...  \n",
       "9  mobil 11 month ? u r entitl updat latest colou...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "      <td>go jurong point , crazi .. avail bugi n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "      <td>ok lar ... joke wif u oni ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "      <td>u dun say earli hor ... u c alreadi say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah n't think goe usf , live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>freemsg hey there darling it's been 3 week's n...</td>\n",
       "      <td>freemsg hey darl 's 3 week 's word back ! 'd l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>even my brother is not like to speak with me. ...</td>\n",
       "      <td>even brother like speak . treat like aid patent .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>as per your request 'melle melle (oru minnamin...</td>\n",
       "      <td>per request 'mell mell ( oru minnaminungint nu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message  \\\n",
       "0      0  go until jurong point, crazy.. available only ...   \n",
       "1      0                      ok lar... joking wif u oni...   \n",
       "2      1  free entry in 2 a wkly comp to win fa cup fina...   \n",
       "3      0  u dun say so early hor... u c already then say...   \n",
       "4      0  nah i don't think he goes to usf, he lives aro...   \n",
       "5      1  freemsg hey there darling it's been 3 week's n...   \n",
       "6      0  even my brother is not like to speak with me. ...   \n",
       "7      0  as per your request 'melle melle (oru minnamin...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  go jurong point , crazi .. avail bugi n great ...  \n",
       "1                      ok lar ... joke wif u oni ...  \n",
       "2  free entri 2 wkli comp win fa cup final tkt 21...  \n",
       "3        u dun say earli hor ... u c alreadi say ...  \n",
       "4         nah n't think goe usf , live around though  \n",
       "5  freemsg hey darl 's 3 week 's word back ! 'd l...  \n",
       "6  even brother like speak . treat like aid patent .  \n",
       "7  per request 'mell mell ( oru minnaminungint nu...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove whitespaces\n",
    "df['tokenized'] = df['tokenized'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "\n",
    " # Remove non-standard characters (e.g., special symbols, non-printable characters)\n",
    "df['tokenized'] = df['tokenized'].str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
    "\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting our X & y\n",
    "X = df.drop(['label', 'message'], axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a CountVectorizer with max_features\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "\n",
    "# fit and transform the corpus using CountVectorizer\n",
    "X_cv = cv.fit_transform(df['tokenized'])\n",
    "\n",
    "# create a tfidfTransformer()\n",
    "tfidf = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = tfidf.fit_transform(X_cv).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       1\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5567    1\n",
       "5568    0\n",
       "5569    0\n",
       "5570    0\n",
       "5571    0\n",
       "Name: label, Length: 5572, dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, BernoulliNB, CategoricalNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score\n",
    "import seaborn as sns\n",
    "\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "cmb = ComplementNB()\n",
    "bnb = BernoulliNB()\n",
    "cnb = CategoricalNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot confusion matrix\n",
    "def plot_conf_mat(conf_mat):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix using seaborns heatmap().\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(3,3))\n",
    "    ax = sns.heatmap(conf_mat,\n",
    "                     center=True,\n",
    "                     annot=True,\n",
    "                     cbar=False,\n",
    "                     fmt='.0f')\n",
    "    plt.xlabel(\"True label\")\n",
    "    plt.ylabel(\"Predicted label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB model provided an accuracy score of 86.46%\n",
      "GaussianNB model provided an precision score of 49.62%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Using the GaussianNB\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_1 = gnb.predict(X_test)\n",
    "\n",
    "# Accuracy Score\n",
    "print(f'GaussianNB model provided an accuracy score of {accuracy_score(y_test, y_pred_1) * 100:.2f}%')\n",
    "# Precision Score\n",
    "print(f'GaussianNB model provided an precision score of {precision_score(y_test, y_pred_1) * 100:.2f}%')\n",
    "\n",
    "# Plot Confusion matrix using seaborn, heatmap\n",
    "conf_mat = confusion_matrix(y_test, y_pred_1)\n",
    "plot_conf_mat(conf_mat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[832, 134],\n",
       "       [ 17, 132]], dtype=int64)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
